{% extends 'base.html' %}

{% block header %}
  <h1>{% block title %}Word2Vec queries for Integrum news corpus mentioning Finland, 1993 - 2017{% endblock %}</h1>
{% endblock %}

{% block content %}
<div class='main'>
  <div class='feature'>
    <a class='teaserlink' href='most_similar'>Find similar words</a>
    <span>Enter one word, or a comma separated list of words to find the most similar words to the ones you entered</span>
  </div>
  <div class='feature'>
    <a class='teaserlink' href='most_similar'>Compare word distances</a>
    <span>Enter two words to see how close or far they are. The closer the distance is to 1, the closer the words are in the corpus.</span>
  </div>
  <div class='feature'>
    <a class='teaserlink' href='most_similar'>Perform word vector arithmetics</a>
    <span>Ask questions of the type is "What is A to B, but not to C". <a href='https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/'>This article</a> gives an an intuition on how word vectors.</span>
  </div>
</div>
<div class='info'>
  <div class='column'>
    <h2>Corpora</h2>
    <p>Models have been trained on 2 corpora of <b>Russian news</b> articles filtered by mention of <b>Finland</b>:
    <ul>
      <li><p><b>Integrum Federal</b> - Federal news media - 1999 - 2017</p>
<table>
<tr><th colspan='3'>Articles per year:</th></tr>
<tr><td><b>1999:</b> 133 </td>  <td><b>2006:</b> 2086</td>  <td><b>2012:</b> 1266</td></tr>    
<tr><td><b>2000:</b> 1394</td>  <td><b>2007:</b> 2063</td>  <td><b>2013:</b> 1222</td></tr>    
<tr><td><b>2001:</b> 1457</td>  <td><b>2008:</b> 2015</td>  <td><b>2014:</b> 1170</td></tr>  
<tr><td><b>2002:</b> 947 </td>  <td><b>2009:</b> 1729</td>  <td><b>2015:</b> 1144</td></tr>  
<tr><td><b>2003:</b> 1191</td>  <td><b>2010:</b> 1411</td>  <td><b>2016:</b> 1009</td></tr>  
<tr><td><b>2004:</b> 1264</td>  <td><b>2011:</b> 1393</td>  <td><b>2017:</b> 1040</td></tr>  
<tr><td><b>2005:</b> 1440</td></tr>
</table>

</li>
      <li><p><b>Integrum Local</b> - Local news media from St. Petersburg and Karelia - 1993 - 2017</p>

<table>
<tr><th colspan='3'>Articles per year:</th></tr>
<tr><td><b>1993:</b> 9   </td><td><b>2002:</b> 1775</td><td><b>2010:</b> 1812</td></tr>
<tr><td><b>1995:</b> 72  </td><td><b>2003:</b> 2009</td><td><b>2011:</b> 1364</td></tr>
<tr><td><b>1996:</b> 173 </td><td><b>2004:</b> 1573</td><td><b>2012:</b> 1477</td></tr>
<tr><td><b>1997:</b> 1142</td><td><b>2005:</b> 1811</td><td><b>2013:</b> 1464</td></tr>
<tr><td><b>1998:</b> 1381</td><td><b>2006:</b> 2230</td><td><b>2014:</b> 1735</td></tr>
<tr><td><b>1999:</b> 1841</td><td><b>2007:</b> 2072</td><td><b>2015:</b> 1620</td></tr>
<tr><td><b>2000:</b> 2000</td><td><b>2008:</b> 2104</td><td><b>2016:</b> 1247</td></tr>
<tr><td><b>2001:</b> 2132</td><td><b>2009:</b> 1790</td><td><b>2017:</b> 1184</td></tr>
</table>


</li>
    </ul>
  </div>
  <div class='column'>
    <h2>Models and Implementation</h2>
    <p>The models used here were trained using <a href='https://radimrehurek.com/gensim/models/word2vec.html'>Gensim</a> implementation of <a href='https://arxiv.org/abs/1301.3781'>Word2Vec</a> (Mikolov et al., 2013).</p> 

    <p>For each corpus, the following models have been trained:
    <ul>
      <li><b>All years</b> - All articles from the entire available time span were used to train this model. Only individual words were used to train (no common phrase detection).</li>
      <li><b>All years / phrases</b> - All articles from the entire available time span were used to train this model, in addition, the model tried to detect common phrases and learn those as well.</li> 
      <li><b>5 year batches</b> - In addition to training on the entire corpus, 5-year batches were used to train the models for 1993-1997, 1994-1999, 1994-1999, etc</li>   
    </ul>
    </p>

    <p>All models were trained for 5 epochs, using <em>CBOW - Continuous Bag of Words</em> setting, training included all words, including ones that occured only once.

    <p>The models were trained on lemmatized news articles, with all punctuation stripped. The lemmatization was done using <a href='https://github.com/nlpub/pymystem3'>Pymystem3</a>, a Python wrapper for <a href='https://tech.yandex.ru/mystem/'>Yandex Mystem</a>.
    </p>
  </div>
</div>

{% endblock %}